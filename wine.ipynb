{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPo0UW68KI+nyWvDxyzb6kq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadiroudsari/wine-detection/blob/master/wine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUPRgfZxlyWD"
      },
      "source": [
        "# Country  of Wine prediction\n",
        "This project was prepered for bigdata course final project. I have done some task on this specific dataset.  \n",
        "From our exploratory data analysis below, we will observed:\n",
        "- Cleaning the dataset \n",
        "- shuffling the data and sample it because the original data set is 129975 row.\n",
        "- Data pre processing  of the descripyion for NLP;\n",
        "- Feature engineering using tf-idf to get numeric value from terms_stemmed column.\n",
        "- Encoding the category column country to index\n",
        "- Training the Multilayer perceptron classifier model\n",
        "- Training the One-vs-Rest classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OB01s3doGls"
      },
      "source": [
        "here i load JDK 8 LTS\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoDZL72roQUF"
      },
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" # Set path to JAVA_HOME\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QckDy6XobDT"
      },
      "source": [
        "## **1.** Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05-AnQfJpTnu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "04dbef29-1c14-42c8-c18f-7f0b0c069ad0"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n",
            "openjdk-8-jdk-headless is already the newest version (8u252-b09-1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmBc4ZP7Ob4F"
      },
      "source": [
        "## **2.** Import useful PySpark packages *italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxh5saK_PE_F"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt9S9qaNRJ2M"
      },
      "source": [
        "\n",
        "\n",
        "# **3.** Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv_PApasRR0C"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\").set('spark.executor.memory', '4G').set('spark.driver.memory', '45G').set('spark.driver.maxResultSize', '10G')\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCaz_DltEKdg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "e04cf8c4-f295-4a72-eb07-7438f76c8376"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://dc6ccb28d348:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc32fa6cb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwKEiYgjOUCW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "9bb9d992-313a-41cb-edc2-9fea0bb8e933"
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.memory', '45G'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.app.id', 'local-1592590277045'),\n",
              " ('spark.driver.maxResultSize', '10G'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('spark.driver.port', '36137'),\n",
              " ('spark.ui.port', '4050'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.executor.memory', '4G'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.driver.host', 'dc6ccb28d348')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb48vfpJOjo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5822a487-ea58-487a-9d5b-f6cd00babc19"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVoMBEE7O6Yi"
      },
      "source": [
        "## **1. Link Google Colab to our Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBGkZlTzO77S"
      },
      "source": [
        "GDRIVE_DIR = \"/content/gdrive\" # Your own mount point on Google Drive\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\" # Your own home directory\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/1898985/dataSet\" # Your own data directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmrL0RVPj1E-"
      },
      "source": [
        "## **1. Link Google Colab to our Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41tfSAQTjlbr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41ad9cc1-1aef-4563-8ef4-60c8a21b5693"
      },
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuhblJO3j2dy"
      },
      "source": [
        "## **2. Data Acquisition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnTyf7YOlYl5"
      },
      "source": [
        "### **Read dataset file into a Spark Dataframe**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEEY66sblZbY"
      },
      "source": [
        "wine_df = spark.read.load(GDRIVE_DATA_DIR, \n",
        "                           format=\"csv\", \n",
        "                           sep=\",\", \n",
        "                           inferSchema=\"true\", \n",
        "                           header=\"true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wy4Msc1mMNq"
      },
      "source": [
        "### **get the overview of dataset rows and columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaVWscgdXAH6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21d7f90a-9699-4fdc-a6ce-b30e97e9e5d8"
      },
      "source": [
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(wine_df.count(), len(wine_df.columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataset is 129975 rows by 14 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokTxkKvoxBm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1fa076b5-c167-437b-b5fb-44348289feca"
      },
      "source": [
        "wine_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- designation: string (nullable = true)\n",
            " |-- points: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- province: string (nullable = true)\n",
            " |-- region_1: string (nullable = true)\n",
            " |-- region_2: string (nullable = true)\n",
            " |-- taster_name: string (nullable = true)\n",
            " |-- taster_twitter_handle: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- variety: string (nullable = true)\n",
            " |-- winery: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5mGqgtB-GAr"
      },
      "source": [
        "*convert pyspark dataframe to panda dataset to shuffle on panda dataset and then turn it back to pyspark dataframe.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBvOCXFGv-xq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "459f2c9a-a0a4-4d19-a862-7f35a909da07"
      },
      "source": [
        "\n",
        "p_wine_df = wine_df.toPandas()\n",
        "p_wine_df=p_wine_df.sample(frac=1)\n",
        "p_wine_df = p_wine_df.iloc[98000:,]\n",
        "wine_df=spark.createDataFrame(p_wine_df)\n",
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(wine_df.count(), len(wine_df.columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataset is 31975 rows by 14 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szn9_suP9L7I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KG_ZZQK-dH0"
      },
      "source": [
        "\n",
        "**Under sample 29975 row from 129975 rows.**\n",
        " \n",
        "\n",
        "- ~Shuffling the data and sample it because the original thata set is 129975 row~.(**DONE**)\n",
        "\n",
        "- Cleaning the dataset \n",
        "- Data pre processing  of the descripyion for NLP;\n",
        "- Feature engineering using tf-idf to get numeric value from terms_stemmed column.\n",
        "- Encoding the category column country to index\n",
        "- Training the Multilayer perceptron classifier model\n",
        "- Training the One-vs-Rest classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYXFUvhXSlAd"
      },
      "source": [
        "# **Cleaing the data set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXw_1Q77ZSVx"
      },
      "source": [
        "Count the number of duplicated description (if any)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUhqqBauW0x9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53a526b6-893e-4f02-9ce8-737c8f481b9b"
      },
      "source": [
        "print(\"The total number of duplicated description are {:d} out of {:d}\".\n",
        "      format(wine_df.count() - wine_df.dropDuplicates(['description']).count(), wine_df.count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of duplicated description are 573 out of 31975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH4vgUAWaASX"
      },
      "source": [
        "### **Remove duplicate description**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCe0mz0DaKwC"
      },
      "source": [
        "wine_df = wine_df.dropDuplicates([\"description\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT4VHkwAaRc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "589fbcf8-86f7-4b2b-b6fd-f976dd83a9c1"
      },
      "source": [
        "print(\"The total number of unique description is: {:d}\".format(wine_df.count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of unique description is: 31402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXLeqKa7atj-"
      },
      "source": [
        "### Check for any missing value (i.e., <code>NULL</code>) along <code>description</code> and country column and Removing the rows that have null description and null country."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQgPSOEEdgWM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d155b0d5-c09e-4e95-94f1-c630254f60e0"
      },
      "source": [
        "wine_df = wine_df.na.drop(subset=[\"country\"])\n",
        "wine_df.where(col(\"country\").isNull()).count()\n",
        "\n",
        "wine_df = wine_df.na.drop(subset=[\"description\"])\n",
        "wine_df.where(col(\"description\").isNull()).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxdlQ4X9l7tq"
      },
      "source": [
        "## The numbers of  Wine's country "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr5sau_1ngDo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "246feb6f-e43d-440b-93b8-3db62d6d16de"
      },
      "source": [
        "wine_df.dropDuplicates(['country']).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ylZD-4ycRx"
      },
      "source": [
        "**A bit Data exploration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtgVbpwF2NRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "ad13ed70-b64a-4b3d-d48e-339b996ba631"
      },
      "source": [
        " for c in wine_df.columns:\n",
        "  print(\"N. of missing values of column `{:s}` = {:d}\".format(c, wine_df.where(col(c).isNull()).count())) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N. of missing values of column `_c0` = 0\n",
            "N. of missing values of column `country` = 0\n",
            "N. of missing values of column `description` = 0\n",
            "N. of missing values of column `designation` = 9031\n",
            "N. of missing values of column `points` = 0\n",
            "N. of missing values of column `price` = 2153\n",
            "N. of missing values of column `province` = 0\n",
            "N. of missing values of column `region_1` = 5015\n",
            "N. of missing values of column `region_2` = 19101\n",
            "N. of missing values of column `taster_name` = 6253\n",
            "N. of missing values of column `taster_twitter_handle` = 7433\n",
            "N. of missing values of column `title` = 1\n",
            "N. of missing values of column `variety` = 1\n",
            "N. of missing values of column `winery` = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmFNGiev3FNv"
      },
      "source": [
        "There is a lot missing value from <code>designation</code>,<code>region_1</code>,<code> region 2 </code>,<code>taster_name</code>,<code>taster_twitter_handle</code> and <code>price</code> columns, so we will drop these columns.also the taster rwitter is not valuable data for us anyway.and in this traing we want to predict the country origin of wine from description, so we just need the description column and country column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_i8WCAS6Uqk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "cb611910-a6d8-4d3c-a0b6-58ae9f3e24e6"
      },
      "source": [
        "columns_to_drop = ['designation', 'region_1','region_2','taster_name','taster_twitter_handle','points','price','province','title','variety','winery']\n",
        "wine_df=wine_df.drop(*columns_to_drop)\n",
        "wine_df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+--------------------+\n",
            "|   _c0| country|         description|\n",
            "+------+--------+--------------------+\n",
            "| 57759|  France|89-91. Barrel sam...|\n",
            "|108609|  France|92–94. Barrel sam...|\n",
            "| 85318|      US|A bit difficult t...|\n",
            "| 62173|Portugal|A blend of Alvari...|\n",
            "| 94149|  Israel|A co-fermented bl...|\n",
            "+------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZZ7OD_BdxT"
      },
      "source": [
        "- ~Shuffling the data and sample it because the original thata set is 129975 row~.(**DONE**)\n",
        "\n",
        "- ~Cleaning the dataset~. (**DONE**)\n",
        "- Data pre processing  of the descripyion for NLP;\n",
        "- Feature engineering using tf-idf to get numeric value from terms_stemmed column.\n",
        "- Encoding the category column country to index\n",
        "- Training the Multilayer perceptron classifier model\n",
        "- Trrainin the One-vs-Rest classifie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmvuUWVPMeaK"
      },
      "source": [
        "# Data Preprocessing\n",
        "in this section I want to use description column that is text we know that for using text data we need to do some preprocessing such as `Text` `cleaning`,`Tokenization`,`Stopwords`, `removal` and `Stemming`.\n",
        "These are the basic of NLP task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTqVo-2-gWp_"
      },
      "source": [
        "def clean_text(df, column_name=\"description\"):\n",
        "    \"\"\" \n",
        "    This fucntion takes the raw text data and apply a standard NLP preprocessing pipeline consisting of the following steps:\n",
        "      - Text cleaning\n",
        "      - Tokenization\n",
        "      - Stopwords removal\n",
        "      - Stemming (Snowball stemmer)\n",
        "\n",
        "    parameter: dataframe\n",
        "    returns: the input dataframe along with the `cleaned_description` column as the results of the NLP preprocessing pipeline\n",
        "\n",
        "    \"\"\"\n",
        "    from pyspark.sql.functions import udf, col, lower, trim, regexp_replace\n",
        "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "    # Text preprocessing pipeline\n",
        "    print(\"***** Text Preprocessing Pipeline *****\\n\")\n",
        "\n",
        "    # 1. Text cleaning\n",
        "    print(\"# 1. Text Cleaning\\n\")\n",
        "    # 1.a Case normalization\n",
        "    print(\"1.a Case normalization:\")\n",
        "    lower_case_news_df = df.select(\"_c0\",\"country\", lower(col(column_name)).alias(column_name))\n",
        "    #lower_case_news_df.show(10,truncate=False)\n",
        "    # 1.b Trimming\n",
        "    print(\"1.b Trimming:\")\n",
        "    trimmed_news_df = lower_case_news_df.select(\"_c0\",\"country\", trim(col(column_name)).alias(column_name))\n",
        "    # trimmed_news_df.show(10,truncate=False)\n",
        "    # 1.c Filter out punctuation symbols\n",
        "    print(\"1.c Filter out punctuation:\")\n",
        "    no_punct_news_df = trimmed_news_df.select(\"_c0\",\"country\", (regexp_replace(col(column_name), \"[^a-zA-Z\\\\s]\", \"\")).alias(column_name))\n",
        "    no_punct_news_df.show(10)\n",
        "    # 1.d Filter out any internal extra whitespace\n",
        "    print(\"1.d Filter out extra whitespaces:\")\n",
        "    cleaned_news_df = no_punct_news_df.select(\"_c0\",\"country\", trim(regexp_replace(col(column_name), \" +\", \" \")).alias(column_name))\n",
        "    # cleaned_news_df.show(10)\n",
        "\n",
        "    # 2. Tokenization (split text into tokens)\n",
        "    print(\"# 2. Tokenization:\")\n",
        "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"tokens\")\n",
        "    tokens_df = tokenizer.transform(cleaned_news_df).cache()\n",
        "    # tokens_df.show(10)\n",
        "\n",
        "    # 3. Stopwords removal\n",
        "    print(\"# 3. Stopwords removal:\")\n",
        "    stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"terms\")\n",
        "    terms_df = stopwords_remover.transform(tokens_df).cache()\n",
        "    # terms_df.show(10)\n",
        "\n",
        "    # 4. Stemming (Snowball stemmer)\n",
        "    print(\"# 4. Stemming:\")\n",
        "    stemmer = SnowballStemmer(language=\"english\")\n",
        "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "    terms_stemmed_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))\n",
        "    terms_stemmed_df.show(10)\n",
        "    \n",
        "    return terms_stemmed_df.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UnrQx--vqzX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "outputId": "64233b4b-1c57-46d9-fabe-d2e0c484fc5c"
      },
      "source": [
        "clean_wine_description_df = clean_text(wine_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Text Preprocessing Pipeline *****\n",
            "\n",
            "# 1. Text Cleaning\n",
            "\n",
            "1.a Case normalization:\n",
            "1.b Trimming:\n",
            "1.c Filter out punctuation:\n",
            "+------+--------+--------------------+\n",
            "|   _c0| country|         description|\n",
            "+------+--------+--------------------+\n",
            "| 57759|  France| barrel sample th...|\n",
            "|108609|  France| barrel sample ti...|\n",
            "| 85318|      US|a bit difficult t...|\n",
            "| 62173|Portugal|a blend of alvari...|\n",
            "| 94149|  Israel|a cofermented ble...|\n",
            "| 27614|  France|a delightful blen...|\n",
            "|126433|      US|a distinctive win...|\n",
            "| 25798|      US|a forestlike eart...|\n",
            "|100855|  France|a fresh pale ros ...|\n",
            "| 64825|      US|a modern popular ...|\n",
            "+------+--------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "1.d Filter out extra whitespaces:\n",
            "# 2. Tokenization:\n",
            "# 3. Stopwords removal:\n",
            "# 4. Stemming:\n",
            "+------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|   _c0| country|         description|              tokens|               terms|       terms_stemmed|\n",
            "+------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "| 57759|  France|barrel sample the...|[barrel, sample, ...|[barrel, sample, ...|[barrel, sampl, t...|\n",
            "|108609|  France|barrel sample tig...|[barrel, sample, ...|[barrel, sample, ...|[barrel, sampl, t...|\n",
            "| 85318|      US|a bit difficult t...|[a, bit, difficul...|[bit, difficult, ...|[bit, difficult, ...|\n",
            "| 62173|Portugal|a blend of alvari...|[a, blend, of, al...|[blend, alvarinho...|[blend, alvarinho...|\n",
            "| 94149|  Israel|a cofermented ble...|[a, cofermented, ...|[cofermented, ble...|[cofer, blend, sy...|\n",
            "| 27614|  France|a delightful blen...|[a, delightful, b...|[delightful, blen...|[delight, blend, ...|\n",
            "|126433|      US|a distinctive win...|[a, distinctive, ...|[distinctive, win...|[distinct, wine, ...|\n",
            "| 25798|      US|a forestlike eart...|[a, forestlike, e...|[forestlike, eart...|[forestlik, earth...|\n",
            "|100855|  France|a fresh pale ros ...|[a, fresh, pale, ...|[fresh, pale, ros...|[fresh, pale, ros...|\n",
            "| 64825|      US|a modern popular ...|[a, modern, popul...|[modern, popular,...|[modern, popular,...|\n",
            "+------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJszlEocnvug"
      },
      "source": [
        "- ~Shuffling the data and sample it because the original thata set is 129975 row~.(**DONE**)\n",
        "\n",
        "- ~Cleaning the dataset~. (**DONE**)\n",
        "- ~Data pre processing  of the descripyion for NLP~.(**DONE**)\n",
        "- Feature engineering using tf-idf to get numeric value from terms_stemmed column.\n",
        "- Encoding the category column country to index\n",
        "- Training the Multilayer perceptron classifier model\n",
        "- Trrainin the One-vs-Rest classifie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXIuSePFwfz"
      },
      "source": [
        "# TF-IDF \n",
        "\n",
        "tf-idf (term frequency-inverse document frequency) is for some how making our words text document into numeric value this is some how feature enginering because we can not use our alphabetic features into our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RWbcjO2Es4y"
      },
      "source": [
        "def extract_tfidf_features(df, column_name=\"terms_stemmed\"):\n",
        "    \"\"\" \n",
        "    This fucntion takes the text data and converts it into a term frequency-inverse document frequency vector\n",
        "\n",
        "    parameter: dataframe\n",
        "    returns: dataframe with tf-idf vectors\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Importing the feature transformation classes for doing TF-IDF \n",
        "    from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
        "    from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "    # USING PIPELINE\n",
        "    cv = CountVectorizer(inputCol=column_name, outputCol=\"tf_features\", vocabSize=100, minDF=8)\n",
        "    # hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=100)\n",
        "    idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[cv, idf]) # replace `cv` with `hashingTF` if needed\n",
        "    features = pipeline.fit(df)\n",
        "    tf_idf_features_df = features.transform(df).cache()\n",
        "\n",
        "    return tf_idf_features_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6uUMuNPFEBC"
      },
      "source": [
        "tf_idf_df = extract_tfidf_features(clean_wine_description_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRXc-FPkFMEW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "28e2320a-589f-404d-8d8d-7e1cec89c378"
      },
      "source": [
        "tf_idf_df.select(col(\"features\")).show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|features                                                                                                                                                                                                                                                               |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(100,[0,2,5,7,32,46,63,73,78],[0.6802593024878884,0.9093669920889772,1.210377992987827,1.4055043731493984,2.275142190192505,2.545913790495969,2.771091930507967,2.910752045073863,2.966740183765621])                                                                  |\n",
            "|(100,[0,2,7,11,37,44,49,62,81,82,99],[1.3605186049757767,0.9093669920889772,1.4055043731493984,1.7878470638367085,2.4994492087700126,2.5194381165119415,2.5618944624843194,2.7449597902311185,2.9962742185120246,2.976696706588424,3.1312644006002075])                |\n",
            "|(100,[0,7,8,25,28,37,53,56,65,81,82],[0.6802593024878884,1.4055043731493984,1.3995442622176646,2.17210996429427,2.2062526832206077,2.4994492087700126,2.641386585147388,2.673192991861442,2.8058014494873635,2.9962742185120246,5.953393413176848])                    |\n",
            "|(100,[2,5,14,16,23,25,32,40,50,54,87,90],[0.9093669920889772,1.210377992987827,1.7796899475643904,1.86380289607891,2.136122012577073,2.17210996429427,2.275142190192505,2.480613364559636,2.592085434763465,2.685736710339972,3.0506603681855777,3.0717567613289134])  |\n",
            "|(100,[0,1,2,4,7,9,11,16,22,25,28,65],[1.3605186049757767,0.6970109632474167,0.9093669920889772,1.1867107499007585,1.4055043731493984,1.4919134151871625,1.7878470638367085,1.86380289607891,2.1105478963740034,2.17210996429427,2.2062526832206077,2.8058014494873635])|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV1D_2hJCvof"
      },
      "source": [
        "- ~Shuffling the data and sample it because the original thata set is 129975 row~.(**DONE**)\n",
        "\n",
        "- ~Cleaning the dataset~. (**DONE**)\n",
        "- ~Data pre processing  of the descripyion for NLP~.(**DONE**)\n",
        "- ~Feature engineering using tf-idf to get numeric value from terms_stemmed column~.(**DONE**)\n",
        "- Encoding the category column country to index\n",
        "- Training the Multilayer perceptron classifier model\n",
        "- Trrainin the One-vs-Rest classifie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16_hLdd2LBzm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "c4dbc4ee-c075-4834-9655-003b7278a8d9"
      },
      "source": [
        "columns_to_drop = ['description', 'tokens','terms','terms_stemmed','tf_features']\n",
        "tf_idf_df=tf_idf_df.drop(*columns_to_drop)\n",
        "tf_idf_df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+--------------------+\n",
            "|   _c0| country|            features|\n",
            "+------+--------+--------------------+\n",
            "| 57759|  France|(100,[0,2,5,7,32,...|\n",
            "|108609|  France|(100,[0,2,7,11,37...|\n",
            "| 85318|      US|(100,[0,7,8,25,28...|\n",
            "| 62173|Portugal|(100,[2,5,14,16,2...|\n",
            "| 94149|  Israel|(100,[0,1,2,4,7,9...|\n",
            "+------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU0NFy6zYGoa"
      },
      "source": [
        "cheack and remove any possible zero length vector  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbBf4gEaYFid"
      },
      "source": [
        "@udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "    return v.numNonzeros()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM84piufZA_D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3b087af-de93-4151-fe4e-79c499fe5440"
      },
      "source": [
        "print(\"Total n. of zero-length vectors: {:d}\".\n",
        "      format(tf_idf_df.where(num_nonzeros(\"features\") == 0).count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total n. of zero-length vectors: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qgALP2lZHkC"
      },
      "source": [
        "removing zero length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQm4u1keZHE8"
      },
      "source": [
        "tf_idf_df = tf_idf_df.where(num_nonzeros(\"features\") > 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmaXl38QZQoE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c162bd4c-b513-4a51-f086-d1a558041a0d"
      },
      "source": [
        "print(\"Total n. of zero-length vectors (after removal): {:d}\".\n",
        "      format(tf_idf_df.where(num_nonzeros(\"features\") == 0).count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total n. of zero-length vectors (after removal): 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldoazscXxBuB"
      },
      "source": [
        "for neural network training we need to our label data be indexed. so I use `StringIndexer`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYz1ziHPCIo7"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"country\", outputCol=\"countryIndex\")\n",
        "tf_idf_df_index = indexer.fit(tf_idf_df).transform(tf_idf_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FHgaBE6Cv84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "8ac5dcd6-0109-4f2e-c683-d2284795f5cf"
      },
      "source": [
        "tf_idf_df_index.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+--------------------+------------+\n",
            "|   _c0| country|            features|countryIndex|\n",
            "+------+--------+--------------------+------------+\n",
            "| 57759|  France|(100,[0,2,5,7,32,...|         1.0|\n",
            "|108609|  France|(100,[0,2,7,11,37...|         1.0|\n",
            "| 85318|      US|(100,[0,7,8,25,28...|         0.0|\n",
            "| 62173|Portugal|(100,[2,5,14,16,2...|         4.0|\n",
            "| 94149|  Israel|(100,[0,1,2,4,7,9...|        13.0|\n",
            "+------+--------+--------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqn0Dnf5DlF4"
      },
      "source": [
        "- ~Shuffling the data and sample it because the original thata set is 129975 row~.(**DONE**)\n",
        "\n",
        "- ~Cleaning the dataset~. (**DONE**)\n",
        "- ~Data pre processing  of the descripyion for NLP~.(**DONE**)\n",
        "- ~Feature engineering using tf-idf to get numeric value from terms_stemmed column~.(**DONE**)\n",
        "- \n",
        "~Encoding the category column country to index.~(**DONE**)\n",
        "- Training the Multilayer perceptron classifier model\n",
        "- Training the One-vs-Rest classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsyr8G9H9bD4"
      },
      "source": [
        "# Randomly split our original dataset `house_df` into 90÷10 for training and test, respectively\n",
        "train_df, test_df = tf_idf_df_index.randomSplit([0.8, 0.2], seed=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbA4Oo3awouH"
      },
      "source": [
        "# Multilayer perceptron classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ptl9vTAd54C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2483021c-6037-4daa-f451-a6e4cb501600"
      },
      "source": [
        "train_df.dropDuplicates(['countryIndex']).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zegtSV-eJXv5"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "layers = [100, 130, 50, train_df.dropDuplicates(['countryIndex']).count()]\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234,featuresCol='features', labelCol='countryIndex')\n",
        "# train the model\n",
        "model = trainer.fit(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsAU19BxklCv"
      },
      "source": [
        "#compute accuracy on the test set\n",
        "result = model.transform(test_df)\n",
        "predictionAndLabels = result.select(\"prediction\", \"countryIndex\")\n",
        "predictionAndLabels = predictionAndLabels.withColumnRenamed(\"countryIndex\", \"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koFlFMJN0FZC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d6e83c3-28ea-48c8-fce0-2d5ce1e09b21"
      },
      "source": [
        "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy = 0.6751761942051684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl85Asid4pQY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e020b4ed-bad3-410d-fcbc-fa8110c68447"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
        "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy = 0.6518270753025871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svnNAbCGHO_7"
      },
      "source": [
        "- ~Shuffling the data and sample it because the original thata set is 129975 row~.(**DONE**)\n",
        "\n",
        "- ~Cleaning the dataset~. (**DONE**)\n",
        "- ~Data pre processing  of the descripyion for NLP~.(**DONE**)\n",
        "- ~Feature engineering using tf-idf to get numeric value from terms_stemmed column~.(**DONE**)\n",
        "- \n",
        "~Encoding the category column country to index.~(**DONE**)\n",
        "- ~Training the Multilayer perceptron classifier model~.(**DONE**)\n",
        "- Training the One-vs-Rest classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1bzZ9xuWZTg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd3JOUTmO6jO"
      },
      "source": [
        "# One-vs-Rest classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW0ihun_Qbaa"
      },
      "source": [
        "train_df = train_df.withColumnRenamed(\"countryIndex\", \"label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3LgzzwO_kL"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
        "\n",
        "# instantiate the One Vs Rest Classifier.\n",
        "ovr = OneVsRest(classifier=lr)\n",
        "\n",
        "# train the multiclass model.\n",
        "ovrModel = ovr.fit(train_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkPgT4g8pAzx"
      },
      "source": [
        "test_df = test_df.withColumnRenamed(\"countryIndex\", \"label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hIC2nhBovwm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a933d43-5d88-4e7e-b467-7148d739238e"
      },
      "source": [
        "predictions = ovrModel.transform(test_df)\n",
        "\n",
        "# obtain evaluator.\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "\n",
        "# compute the classification error on test data.\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.410765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMA8XbOhHnth"
      },
      "source": [
        "- ~Shuffling the data and sample it because the original thata set is 129975 row~.(**DONE**)\n",
        "\n",
        "- ~Cleaning the dataset~. (**DONE**)\n",
        "- ~Data pre processing  of the descripyion for NLP~.(**DONE**)\n",
        "- ~Feature engineering using tf-idf to get numeric value from terms_stemmed column~.(**DONE**)\n",
        "- \n",
        "~Encoding the category column country to index.~(**DONE**)\n",
        "- ~Training the Multilayer perceptron classifier model~.(**DONE**)\n",
        "- ~Trraining the One-vs-Rest classifier~.(**DONE**)"
      ]
    }
  ]
}